---
title: "W203 Final"
author: "Mohammad Jawad Habib"
date: "April 21, 2016"
output: pdf_document
---

# Part 1: Multiple Choice
Q1. d, ANOVA (to test the significance of OLS)

Q2. b, All coefficients for each independent variable equal zero (this is the null hypothesis of ANOVA for OLS)

Q3. e, None of the above
Notes on Q3: 
--- c (b1 is not equal to zero) is NOT correct. ANOVA doesn't tell us which specific b is not equal to zero. It only tells us that one of the coefficients is not equal to zero, not which specific one.
--- d (the relationship in the underlying population is linear) is NOT correct. This is an assumption of the linear regression, not a result from significant ANOVA of linear regression. A significant ANOVA for OLS only tells us that at least one of the independent variable is linearly related to the dependent variable but not which specific independent variable.


Q4. b, Maximum number of pushups in 3 minutes (because it might be correlated
with tricep strength which is already in our model)

Q5. d,
because we did not ourselves subject people to this torment (um, treatment), it happened naturally. Further, if there were online daters in the country before ISP blocked access, we can now see how their relationsip satisfaction changes.

Q6. c, Regression assumptions that have been met

Q7. b, The power of the test (is not a random variable, it's calculated)

Q8. c, Insignificant results are commonly relegated to the file drawer 
If this were actually true, only 5% or less of the published research would not be reproducible. So relegating insignificant research to the file drawer does not help to explain why more than 5% of the published results appear to be type-1 errors.
Notes on Q8: 
--- a, certainly explains that some studies could have obtained significant p-values randomly. These random results would not be reproducible.
--- b, some researchers could have exploited degrees of freedom to obtain more significant results e.g. stopping data collection at a convenient time, or using only parts of the data or variables, or using a sub-par test of significance for the given situation.
--- d, as you test more hypothesis, the probability of finding a significant result due to chance also increases


# Part 2. Test Selection
Q9. a, chi-square (use_reddit ~ marital_status), categorilcal, nominal
Notes on Q9: 
- use_reddit is a categorical variable with two categories (Considering "Refused"" and "No"" as missing values).
- marital_status is a categorical variable with 7 categories. These are not ordinal, nor interval. Just names of categories that cannot be ordered.

Given the above info about the variables:
- t-test cannot be used because it requires numeric data
- Pearson Correlation cannot be used because it requires two variables from a Gaussian distribution
- Wilcoxon signed rank test requires same participants in different measures and it is limited to two groups.

Q10. d, ANOVA (life_quality ~ region)
Notes on Q10: 
- region is a categorical variable with 4 categories.
- life_quality is an ordinal variable with 5 levels (Excellent to Poor). We cannot assume that it is interval with the information given.
- we can assume that different people are in different categories on both region and life_quality

- Pearson correlation cannot be used because it requires two variables from a Gaussian distribution
- Wilcoxon signed rank test requires same participants in different measures; limited to two groups
- Binary logistic regression requires exactly two categories in outcome variable
That leaves us with the controversial choice of ANOVA on ordinal outcome. Robust ANOVA/Kruskal Wallis test would be better but that is not one of the options.

Q11. b, t-test (years_in_relationship ~ flirted_online)
Notes: 
- flirted_online is a categorical variable with two categories
- years_in_relationship is a ratio variable: there's a true zero and 2-1 = 3-2.

- Fisher's exact test requires categorical variables.
- Wilcoxon signed rank test requires same participants in different measures; limited to two groups
- ANOVA is used when there are more than two categories and the dependent variable is normally distributed.
In fact, we should use an independent bootstrapped t-test or Mann Whitney Test (aka Wilcoxon Rank Sum Test) in this case because we do not have homogeneity of variance between the two flirted_online groups (leveneTest).

Q12. a, chi-square (lgbt ~ adults_in_household)
Notes:
- lgbt is a categorical variable with six categories
- adults_in_household is recorded as an ordinal variable because 6 or more is considered one category.
- ANOVA cannot be used because it requires a normally distributed dependent variable
- Pearson correlation cannot be used because it requires two variables from a Gaussian distribution
- Wilcoxon rank sum test requires exactly two groups

Q13. a, Pearson Correlation (totalchildren ~ age)

Q14. b, Wilcoxon Rank-Sum Test (number of children ~ gender of people aged 31)

# Part 3. Data Analysis
```{r}
dat <- read.csv("Dating.csv", header = TRUE, stringsAsFactors = FALSE)
```

## 15. OLS Regression

### a. Mean of life_quality
```{r}
unique(dat$life_quality)
# omit "Refused" and "Don't know" and calculate mean

lq <- dat$life_quality[!dat$life_quality %in% c("Refused", "Don't know")]
lq <- lq[!is.na(lq)]
lq <- as.numeric(as.character(lq))
mean(lq) # 2.607079

```

### b. Mean of years_in_relationship
```{r}
unique(dat$years_in_relationship)
# omit "Refused", "Don't know" and " "
# leave "0" in there

yr <- dat$years_in_relationship[!dat$years_in_relationship %in% c("Refused", "Don't know", " ")]
yr <- yr[!is.na(yr)]
yr <- as.numeric(as.character(yr))
mean(yr) # 13.47697
```

### c. Create a subset of data omitting missing values
```{r}
ds1 <- dat[, c("life_quality", "years_in_relationship", "use_internet")]
# apply(ds1, 2, class)
# apply(ds1, 2, levels)


# check what's in use_internet
unique(ds1$use_internet)

# keep only complete cases
ds1 <- apply(ds1, 2, function(x) {ifelse(x %in% c("Refused", "Don't know", " "), NA, x)})
ds1 <- ds1[complete.cases(ds1), ]
nrow(ds1) #1090 cases left
ds1 <- data.frame(ds1, stringsAsFactors = FALSE) # convert to data.frame
ds1[, 1] <- as.numeric(ds1[, 1])
ds1[, 2] <- as.numeric(ds1[, 2])

```

### d. Fit an OLS model: life_quality ~ years_in_relationship
```{r}
lm1 <- lm(life_quality ~ years_in_relationship, data = ds1)

lm1s <- summary(lm1)
lm1s

lm1s$coefficients[1] # intercept is 2.66978, p < 0.001, statistically significant
# Intercept tells us the expected value of life_quality without any independent variable
# So our intercept is very close to the mean value of life_quality which makes sense

lm1s$coefficients[2] # slope is -0.00499, p < 0.05, statistically significant
# however, life_quality only decreases by 0.00498 for 1 year increase in years_in_relationship
# this is a very small effect

lm1s$r.squared # R-squared = 0.00586, i.e. 0.58% of variance in life_quality explained by years_in_relationship

sqrt(lm1s$r.squared) # Pearson's R = 0.07655 (< 0.30) which points to no linear correlation
# This does not appear to be practically significant

cbind("f ratio" = lm1s$fstatistic[1], "critical f value" = qf(c(0.975), 1, 1088)) # F-ratio = 6.41427 
# F-ratio obtained before is greater than the critical value
pf(lm1s$fstatistic[1], lm1s$fstatistic[2], lm1s$fstatistic[3], lower.tail = FALSE) * 2 # p < 0.05


```

### e. Fit a second OLS model
```{r}
lm2 <- lm(life_quality ~ years_in_relationship + use_internet, data = ds1)
lm2s <- summary(lm2)
lm2s

# slope coefficient for use_internet
lm2s$coefficients[3] # slope is -0.40374, p < 0.01, statistically significant
lm2s$adj.r.squared # Adjusted R-squared = 0.02282, i.e. 2.28% of variance in life_quality explained by years_in_relationship & use_internet together
sqrt(lm2s$adj.r.squared) # Pearson's R = 0.15105 (< 0.30) which points to no linear relationship
# This does not appear to be practically significant

cbind("f ratio" = lm2s$fstatistic[1], "critical f value" = qf(c(0.975), 1, 1088)) # F-ratio = 13.71293 
# F-ratio obtained before is greater than the critical value
pf(lm2s$fstatistic[1], lm2s$fstatistic[2], lm2s$fstatistic[3], lower.tail = FALSE) * 2 # p < 0.01

```

### f. F-ratio and p-value between two OLS models
```{r}
lmcompare <- anova(lm1, lm2)
lmcompare

```
Using years_in_relationship and use_internet together improved the fit of the model to the data compared to years_in_relationship alone. F(1, 1087) = 20.894, p < 0.01.

## Q16. Logistic Regression

### a. Odds that a responded has flirted online
```{r}

ds2 <- dat[, c("usr", "flirted_online")]
nrow(ds2)

unique(ds2$flirted_online)
unique(ds2$usr)

ds2 <- apply(ds2, 2, function(x) {ifelse(x %in% c("Refused", "Don't know", " "), NA, x)})
ds2 <- ds2[complete.cases(ds2), ]
nrow(ds2) #1885 cases left
ds2 <- data.frame(ds2, stringsAsFactors = FALSE) # convert to data.frame

library(gmodels)
ct <- CrossTable(ds2$flirted_online)
ct$prop.row[2] / ct$prop.row[1] # odds ratio in sample (flirted / not flirted) = 0.26087

```

### b. Logistic regression: flirted_online ~ usr
```{r}
# convert flirted_online to factor
# ds2$flirted_online <- factor(ds2$flirted_online)
# ds2$flirted_online <- relevel(ds2$flirted_online, ref = "No") # convert no to base

ds2$flirted <- ifelse(ds2$flirted_online == "Yes", 1, 0)
ds2$usr <- factor(ds2$usr) # rural is the base factor

gm <- glm(flirted ~ usr, data = ds2, family = binomial)
gm
gms <- summary(gm)
gms

# Akaike Information Criterion
gms$aic # AIC = 1909.359
```

### c. Odds of urban user flirting online vs. rural user
```{r}

# get odds ratio from the model
exp(gm$coefficients) # Urban user is 2.10 times more likely than rural user to flirt online in sample


# we can use the model to predict the probability of flirting online and get odds ratio for that too
# use type = "response"" to get probabilities instead of log-odds
pflirt.rural <- predict.glm(gm, newdata = data.frame("usr" = "Rural"), type = "response") 
pflirt.rural

pflirt.urban <- predict.glm(gm, newdata = data.frame("usr" = "Urban"), type = "response")
pflirt.urban
pflirt.odds <- pflirt.urban / pflirt.rural
pflirt.odds # Urban user is 1.83 times more likely to flirt than rural user

```